<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lesson 3: Network Plasticity</title>
    <style>
      body {
        font-family: Inter, system-ui, sans-serif;
        background: #0f1419;
        color: #e6e9ef;
        margin: 0;
        padding: 20px;
        line-height: 1.6;
      }
      h1,
      h2,
      h3 {
        color: #ff9bf0;
        margin-top: 24px;
      }
      h1 {
        font-size: 28px;
        margin-bottom: 16px;
      }
      h2 {
        font-size: 22px;
        margin-bottom: 12px;
      }
      ul,
      ol {
        margin-left: 20px;
      }
      li {
        margin-bottom: 8px;
      }
      strong {
        color: #ff9bf0;
      }
      p {
        margin-bottom: 12px;
      }
    </style>
  </head>
  <body>
    <h1>Lesson 3: Network Plasticity</h1>

    <p>
      The ability of neural networks to modify their connections over time is
      the foundation of learning and memory. This dynamic property allows
      networks to adapt to new information and optimize their performance.
    </p>

    <h2>Types of Plasticity</h2>

    <h3>Hebbian Learning</h3>
    <p>
      The principle that "neurons that fire together, wire together." When two
      connected neurons are active simultaneously, their connection strengthens.
      This is visible in our simulation when you see clusters of neurons firing
      in synchrony.
    </p>

    <h3>Long-term Potentiation (LTP)</h3>
    <p>
      Persistent strengthening of synapses based on recent patterns of activity.
      Frequently used connections become stronger and more influential in
      network dynamics.
    </p>

    <h3>Long-term Depression (LTD)</h3>
    <p>
      The weakening or elimination of rarely used connections. This prevents the
      network from becoming oversaturated and helps maintain efficient
      information processing.
    </p>

    <h2>Homeostatic Mechanisms</h2>

    <h3>Stability vs. Adaptability</h3>
    <p>
      Networks must balance the ability to learn new patterns while maintaining
      stable operation. Our simulation demonstrates this through:
    </p>
    <ul>
      <li><strong>Voltage decay:</strong> Prevents runaway excitation</li>
      <li><strong>Pulse decay:</strong> Limits the duration of activity</li>
      <li><strong>Threshold mechanisms:</strong> Control when neurons fire</li>
    </ul>

    <h3>Network Dynamics</h3>
    <p>Observe how changing parameters affects network behavior:</p>
    <ul>
      <li>
        <strong>Connection Probability:</strong> Higher values create more
        plastic networks
      </li>
      <li>
        <strong>Network Size:</strong> Larger networks show more complex
        adaptive patterns
      </li>
      <li>
        <strong>Firing Rates:</strong> Affect the speed of plastic changes
      </li>
    </ul>

    <h2>Emergent Behaviors</h2>

    <p>Watch for these signs of network plasticity:</p>
    <ul>
      <li>
        <strong>Cluster Formation:</strong> Groups of neurons that tend to fire
        together
      </li>
      <li>
        <strong>Oscillatory Patterns:</strong> Rhythmic activity that emerges
        from network structure
      </li>
      <li>
        <strong>Cascade Effects:</strong> How single spikes can trigger
        network-wide activity
      </li>
      <li>
        <strong>Adaptation:</strong> Changes in activity patterns over time
      </li>
    </ul>

    <h2>Real-world Applications</h2>

    <p>Network plasticity principles are used in:</p>
    <ul>
      <li>Learning algorithms for AI systems</li>
      <li>Understanding brain development and adaptation</li>
      <li>Designing neuromorphic hardware</li>
      <li>Modeling memory formation and retrieval</li>
    </ul>
  </body>
</html>
