<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lesson 4: Pattern Recognition and Memory</title>
    <style>
        body {
            font-family: Inter, system-ui, sans-serif;
            background: #0f1419;
            color: #e6e9ef;
            margin: 0;
            padding: 20px;
            line-height: 1.6;
        }
        h1, h2, h3 { color: #ff9bf0; margin-top: 24px; }
        h1 { font-size: 28px; margin-bottom: 16px; }
        h2 { font-size: 22px; margin-bottom: 12px; }
        ul, ol { margin-left: 20px; }
        li { margin-bottom: 8px; }
        strong { color: #ff9bf0; }
        p { margin-bottom: 12px; }
    </style>
</head>
<body>
    <h1>Lesson 4: Pattern Recognition and Memory</h1>
    
    <h2>Emergent Computation</h2>
    <p>When properly organized, neural networks can recognize patterns and recall memories from partial cues. This lesson demonstrates associative memory, pattern completion, and the computational power that emerges from simple neural interactions.</p>
    
    <h2>Pattern Storage Mechanism</h2>
    <p>Specific patterns of neural activity can be "stored" in the network through distributed synaptic changes:</p>
    <ul>
        <li><strong>Activation Pattern:</strong> A specific subset of neurons firing together in a coordinated sequence</li>
        <li><strong>Strengthened Connections:</strong> Synapses between pattern neurons become stronger through Hebbian learning</li>
        <li><strong>Memory Trace:</strong> The pattern leaves a lasting change in network connectivity</li>
        <li><strong>Distributed Storage:</strong> Memory is spread across many synapses, not localized to single neurons</li>
        <li><strong>Redundant Encoding:</strong> Multiple pathways can store the same information</li>
    </ul>
    
    <h2>Pattern Recall Process</h2>
    <p>Once stored, patterns can be recalled through several mechanisms:</p>
    <ul>
        <li><strong>Partial Cue:</strong> Activating just part of the original pattern</li>
        <li><strong>Pattern Completion:</strong> The network automatically fills in missing pieces</li>
        <li><strong>Attractor Dynamics:</strong> Activity converges to the nearest stored pattern</li>
        <li><strong>Content-Addressable Memory:</strong> Any part of the pattern can retrieve the whole</li>
        <li><strong>Graceful Degradation:</strong> System works even with damaged connections</li>
    </ul>
    
    <h2>Types of Memory Networks</h2>
    <ul>
        <li><strong>Hopfield Networks:</strong> Store fixed-point attractors, good for pattern completion</li>
        <li><strong>Associative Memory:</strong> Link different patterns together (A→B associations)</li>
        <li><strong>Auto-associative Memory:</strong> Complete partial patterns (A→A completion)</li>
        <li><strong>Sequence Memory:</strong> Store temporal patterns and sequences</li>
        <li><strong>Sparse Coding:</strong> Efficient representation using few active neurons</li>
    </ul>
    
    <h2>Memory Capacity</h2>
    <p>Network memory capacity depends on several factors:</p>
    <ul>
        <li><strong>Number of neurons:</strong> More neurons = more storage capacity</li>
        <li><strong>Connectivity density:</strong> Optimal connectivity for maximum storage</li>
        <li><strong>Pattern sparsity:</strong> Fewer active neurons per pattern = more patterns stored</li>
        <li><strong>Pattern similarity:</strong> Similar patterns interfere with each other</li>
        <li><strong>Learning rate:</strong> Trade-off between learning speed and capacity</li>
    </ul>
    
    <h2>Experimental Protocol</h2>
    <ol>
        <li>Click <strong>"Inject Pattern"</strong> to store a specific activity pattern in the network</li>
        <li>Watch how the pattern spreads and strengthens connections between active neurons</li>
        <li>Wait for network activity to settle back to baseline</li>
        <li>Click <strong>"Test Memory"</strong> to provide a partial cue (subset of original pattern)</li>
        <li>Observe whether the network recalls and completes the full stored pattern</li>
        <li>Try multiple pattern injections to see interference and competition effects</li>
        <li>Experiment with different network parameters to optimize memory performance</li>
    </ol>
    
    <h2>What to Observe</h2>
    <ul>
        <li><strong>Pattern propagation:</strong> How quickly the injected pattern spreads through connected neurons</li>
        <li><strong>Self-sustaining activity:</strong> Whether the pattern becomes temporarily self-maintaining</li>
        <li><strong>Completion dynamics:</strong> How partial cues trigger recall of full patterns</li>
        <li><strong>Pattern interference:</strong> Competition between different stored patterns</li>
        <li><strong>Memory stability:</strong> How long patterns remain retrievable</li>
        <li><strong>Network capacity:</strong> Maximum number of patterns that can be stored</li>
    </ul>
    
    <h2>Biological Memory Systems</h2>
    <p>Real brain memory involves multiple systems:</p>
    <ul>
        <li><strong>Hippocampus:</strong> Rapid learning of new episodic memories</li>
        <li><strong>Neocortex:</strong> Long-term storage of semantic knowledge</li>
        <li><strong>Cerebellum:</strong> Motor learning and procedural memory</li>
        <li><strong>Amygdala:</strong> Emotional memory and fear conditioning</li>
        <li><strong>Striatum:</strong> Habit formation and reward learning</li>
    </ul>
    
    <h2>Memory Phenomena</h2>
    <p>Networks exhibit interesting memory behaviors:</p>
    <ul>
        <li><strong>Priming:</strong> Previous patterns influence current processing</li>
        <li><strong>Interference:</strong> Similar patterns disrupt each other</li>
        <li><strong>Consolidation:</strong> Memories become more stable over time</li>
        <li><strong>Forgetting:</strong> Unused patterns gradually fade</li>
        <li><strong>False memories:</strong> Reconstruction can create non-existent patterns</li>
    </ul>
    
    <h2>Real-World Applications</h2>
    <p>These principles are used in:</p>
    <ul>
        <li><strong>Artificial Neural Networks:</strong> Deep learning for pattern recognition</li>
        <li><strong>Computer Vision:</strong> Image classification and object recognition</li>
        <li><strong>Natural Language Processing:</strong> Text understanding and generation</li>
        <li><strong>Recommender Systems:</strong> Pattern matching for personalization</li>
        <li><strong>Medical Diagnosis:</strong> Pattern recognition in medical data</li>
        <li><strong>Brain-Computer Interfaces:</strong> Decoding intended movements from neural signals</li>
    </ul>
    
    <h2>Memory Disorders</h2>
    <p>Understanding neural memory helps explain disorders:</p>
    <ul>
        <li><strong>Alzheimer's Disease:</strong> Progressive loss of memory networks</li>
        <li><strong>Amnesia:</strong> Specific memory formation or retrieval deficits</li>
        <li><strong>PTSD:</strong> Overstrength traumatic memory traces</li>
        <li><strong>Dementia:</strong> Widespread memory network degeneration</li>
    </ul>
    
    <h2>Try This</h2>
    <ul>
        <li>Can you store multiple patterns without interference?</li>
        <li>What's the minimum cue size needed for pattern recall?</li>
        <li>How do network parameters affect memory capacity?</li>
        <li>Can you create hierarchical patterns (patterns within patterns)?</li>
        <li>What happens when you try to store conflicting patterns?</li>
    </ul>
</body>
</html>
